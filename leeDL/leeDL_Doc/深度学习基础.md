### 局部最小值与鞍点
场景：模型一开始就训练不起来，怎么调参都没用

#### 临界点及其种类
- 局部最小值
- 鞍点：梯度为0，不是局部最小值点；在一个方向最小，其他方向不一定是且一定有一个方向不是最小值点

#### 临界点种类判断
对于给定的一组参数$\vec{\theta}'$, 它附近的损失函数可以用泰勒展开
$$\begin{align}
L(\vec{\theta}) &= L(\vec{\theta}')+(\vec{\theta}-\vec{\theta}')^{T}\vec{g}+\frac{1}{2}(\vec{\theta}-\vec{\theta}')^{T}\mathbf{H}(\vec{\theta}-\vec{\theta}') \\
\end{align}$$
- H是hessian矩阵
在临界点，梯度g为0，令$\vec{v}=(\vec{\theta}-\vec{\theta}')$ ，有
$$\begin{align}
L(\vec{\theta}) &= L(\vec{\theta}')+\frac{1}{2}\vec{v}^{T}\mathbf{H}\vec{v}
\end{align}$$
所以有
$$\begin{align}
\begin{cases}
L(\vec{\theta})>L(\vec{\theta}'), & \vec{v}^{T}\mathbf{H}\vec{v}>0 \\
L(\vec{\theta})<L(\vec{\theta}'), & \vec{v}^{T}\mathbf{H}\vec{v}<0 \\
\end{cases}
\end{align}$$
如果$\vec{v}^{T}\mathbf{H}\vec{v}$ 有时候大于0 ，有时候小于0，就说明不是极大值点，也不是极小值点，是鞍点。
可以直接通过H的特征值来判断$\vec{v}^{T}\mathbf{H}\vec{v}$ 的正负：
- 如果H的特征值都是正的，则H是正定的，$\vec{v}^{T}\mathbf{H}\vec{v}$ 大于0，临界点是极小值
- 如果H的特征值都是负的，则H是负定的，$\vec{v}^{T}\mathbf{H}\vec{v}$ 小于0，临界点是极大值
- 如果H有正有负，则H是不定的，临界点就是鞍点

##### 对于鞍点，H还可以指出脱离鞍点的方向：
如果有一个特征值$\lambda_{1}$ ，对应的特征向量$\vec{x}_{1}$ ，令$\vec{x}_{1}=\vec{\theta}-\vec{\theta}'$ ，有
$$\begin{align}
\vec{x}_{1}^{T}\mathbf{H}\vec{x}_{1} &= \vec{x}_{1}^{T}(\lambda_{1}\vec{x}_{1}) \\
 &= \lambda_{1}\lVert \vec{x}_{1} \rVert ^{2}
\end{align}$$
**所以，如果特征值是小于0的，沿它对应的特征向量方向可以减小损失**
但是求H要计算二次微分，一般不这么干

#### 走出鞍点
大部分时候都是遇到鞍点（那么多参数，一起梯度都大于0概率太小了）

### 批量和动量
在每个回合开始之前打乱数据，重新划分批量
#### 批量大小对梯度下降法的影响
大批量：
- 参数更新次数少，训练快，参数更新比较稳定，但是更难跳出局部最小值点
小批量：
- 参数更新次数多，训练慢，但是更容易跳出局部最小值点，只有出现局部最小平面才会难以跳出
- 对于批量大小为1的梯度下降，会引入随机噪声，更容易跳出局部最小值点了
比较：
- 就算大批量比小批量有相同的训练准确率，测试准确率也是小批量低，这代表过拟合了

#### 动量法
当前参数移动的方向取决于：
- 当前梯度的方向
- 前一步移动的方向
有
$$\begin{align}
\vec{m}_{2} &= \lambda \vec{m}_{1}-\eta \vec{g}_{1}
\end{align}$$
这样参数每一步移动收到了之前所有梯度的影响，影响的权重和$\lambda$ 有关，这也是一个超参数

### 自适应学习率
学习率太大，导致参数在峡谷两侧振荡，此时梯度并未很小

#### AdaGrad
根据梯度调节学习率：
- 梯度越小学习率越大
$$\begin{align}
\vec{\theta}_{t+1}^{i} \leftarrow \vec{\theta}_{t}^{i}-\frac{\eta}{\sigma _{t}^{i}}\vec{g}^{i}
\end{align}$$
其中调节参数$\sigma ^{i}_{t}$ 表示：
- 第i个参数在第t次更新时使用的调节参数
一般使用均方根计算$\sigma _{t}^{i}$ 
$$\begin{align}
\sigma _{1}^{i} &= \sqrt{ \frac{1}{2}[(\vec{g}^{i}_{0 })^{2}+(\vec{g}^{i}_{1})^{2}] } \\
\sigma _{t}^{i} &= \sqrt{ \frac{1}{t+1}\sum_{i=0}^{t} (\vec{g}_{t}^{i})^{2} }
\end{align}$$
#### RMSProp
相比于AdaGrad，可以手动调节当前时刻梯度的重要性
$$\begin{align}
\sigma ^{i}_{t} &= \sqrt{ \alpha(\sigma ^{i}_{t-1})^{2}+(1-\alpha)(\vec{g}^{i}_{t})^{2} }
\end{align}$$
#### Adam
RMSProp加上动量

#### 学习率调度
随着参数更新，让学习率$\eta$ 本身变小，因为$\sigma$ 会产生累积
##### 预热：
让学习率先变大，再变小
为什么预热：
- 因为$\sigma$ 是一个统计结果，需要比较多的数据才能比较准确，一开始数据少所以很容易不准确

#### 总结
- 自适应学习率，梯度大的地方学习率小，梯度小的地方学习率大
- 更进一步可以手动决定当前时刻的梯度对学习率的权重
- 最后是加上动量成为最终版

### 分类
输入一个特征向量，输出一个向量，输出向量的元素就是各个类的概率

#### softmax
将输出归一化，使输出值都为正并且在0和1之间
$$\begin{align}
y_{i}' &= \frac{e^{ y_{i} }}{\sum_{j}^{} e^{ y_{j} }}
\end{align}$$
**在只有两个分类时，softmax和sigmoid等价**
$$\begin{align}
suppose:z_{1} &= z \\
z_{2} &= 0 \\ 
softmax(z) &= \frac{e^z}{e ^{z}+e ^{0}}  \\
 &= \sigma(z)
\end{align}$$
#### 分类损失
整体流程：
- 输入$\vec{x}$ 到网络中，产生$\hat{\vec{y}}$ 
- 经过softmax，得到$\vec{y}'$ 
计算损失，可以用均方差
$$\begin{align}
e &= \sum_{i}^{} (y_{i} -y'_{i})^{2}
\end{align}$$
更多的是用**交叉熵**
$$\begin{align}
e &= -\sum_{i}^{} y_{i}\ln y_{i}' 
\end{align}$$
举个例子
$$\begin{align}
suppose:\vec{y} &= \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix} \\
\hat{\vec{y}} &= \begin{bmatrix}
0.1 \\
0.8 \\
0.1
\end{bmatrix} \\
Los s  &=  -((0\cdot \log(0.1))+1\cdot \log(0.8)+0\cdot \log(0.1))  \\
 &= -\log(0.7) \\
  & \approx  0.223
\end{align}$$
**交叉熵比均方误差好**
- 在损失很大的地方，均方误差很平坦，会导致参数更新慢

### 批量归一化
我们算的损失函数是参数的函数，决定参数对损失影响大小的是特征数据，批量归一化就是将特征数据进行归一化，这样特征数据就不会相差很大，对损失的影响也就不会相差很大
做法：
- 对所有特征向量的同一维度（这里取i）求平均值$m_{i}$ 和标准差$\sigma _{i}$ 
- 然后用每一个特征数据的第i个维度减去平均值再除以标准差
$$\begin{align}
\tilde{x_{i}^{r}} &= \frac{x_{i}^{r} - m_{i}}{\sigma _{i}} 
\end{align}$$
#### 深度学习中的归一化
可以在激活函数前或后做归一化
- 如果选sigmoid，需要激活函数前做归一化，因为sigmoid在0附近斜率大，加快训练速度
每一层之前都做归一化，因为当前输出也相当于之后的特征输入。
批量归一化比较适合大批量，因为大批量可以看做全体样本的采样
##### 一般还会做
$$\begin{align}
\hat{\vec{z}} &= \gamma \odot \tilde{\vec{z}^{i}} +\beta
\end{align}$$
- $\gamma$ 和$\beta$ 都是参数
- 归一化之后$\tilde{z}$ 一定是0，可能会有一些负面影响，把$\gamma ~\beta$ 加回去让隐藏层输出平均值不是0
- 一开始让$\beta$ 为0向量，$\gamma$ 为全1向量

#### 测试时的批量归一化
测试时甚至没有批量，还用移动平均代替
$$\begin{align} 
\bar{\vec{\mu}}^{t} &= p\bar{\vec{\mu}}^{t-1} + (1-p)\vec{\mu}^{t}
\end{align}$$
#### 内部协变量偏移
定义：由于前一层的参数更新导致当前层的输入分布发生变化，这导致每一层在训练的时候，要适应前一层的变化导致的输入分布的变化，增加了训练难度
批量归一化不一定能改善内部协变量偏移，但是能改变误差表面
