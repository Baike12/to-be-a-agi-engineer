### 网络剪枝
#### 彩票假说
- 我们有一个大的网络M1，训练出一个好的模型PM1
- 然后把大的网络神经元剪枝成m1
- 把m1的参数重新初始化，训练不起来
- 但是把m1用PM1的对应参数初始化，就可以训练起来

#### 彩票假说规律
- 训练前后权重绝对值差距越大，保留这些权重能保留更好的模型效果
- 参数初始化的正负号是关键的

### 知识蒸馏
- 教师网络M1，学生网络m1
**有效性解释**：教师网络可以给学生网络输出一些额外的信息
#### softmax添加温度T
$$\begin{align}
y_{i}'  &= \frac{e^{ y_{i}/T  }}{\sum_{j}^{} e^{ y_{j}/T }}
\end{align}$$
- $T>1$ 时相当于做了一个平滑
教师网络添加温度之后分布比较平滑，可以给学生网络输出更多信息
但是温度不宜太大，太大导致分布过于平滑，几乎都无法分类了

### 参数量化
#### 参数量化
#### 权重聚类
- 将值属于某个范围的参数用这个范围内的值代替，然后每个参数只需要用类别表示
	- 类别表示比数值表示一般需要更少的内存，所以可以降低参数内存占用

### 网络架构设计
#### 深度卷积
- 深度卷积：滤波器和输入通道数量一样，一个滤波器只负责一个通道
- 点卷积：核大小为$1\times 1$ 
#### 参数量比较
假设卷积核$k\times k$ ，输入通道$I$ ，输出通道$O$ 
- 一般卷积参数量：$k\times k\times I\times O$ 
- 深度卷积参数量：$k\times k\times i+I\times O$ 
$$\begin{align}
\frac{{k\times k\times I+I\times O}}{k\times k\times I\times O}  &= \frac{1}{O}+\frac{1}{k\times k}
\end{align}$$
- O一般比较大，所以$\frac{1}{O}$ 约等于0
- $\frac{1}{k\times k}$ 是主要的比例

#### 低秩近似
- 假设输入有M个神经元，输出有N个神经元，中间的矩阵W就有$M\times N$ 个参数，如果M或者N很大，参数量就很大
- 在中间插K个神经元的一层，参数量就变成$M\times K+K\times N$ ，如果K比较小，就可以让参数量减小
- 但是会影响W的表现能力，W的秩不再是小于等于$min(M,N)$ ，而是小于K
**深度卷积也可以看成像低秩一样的分层**

### 动态计算
#### 动态调整深度
计算资源不足的时候在某一个中间层就输出，或者跳过某一些层
训练方法：
- 将每一层的输出和标签的交叉熵加起来得到损失
	- $L=e_{1}+e_{2}+\dots$
#### 动态调整层的宽度
训练方法：
- 设置一些层，样本进来的时候不同的宽度都有输出，把这些输出和标签做交叉熵，然后把这些交叉熵相加得到损失，最小化损失
**对一些难易程度不同任务，模型可以自己决定任务占用的计算资源的多少**